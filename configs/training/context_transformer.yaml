# Context-Augmented Transformer Configuration
# ===========================================
# Neural classifier using pretrained transformer with contextual input.
# Input format: [ROLE] [SEP] [TOOL] [SEP] [SCHEMA] [SEP] [DESC] [SEP] [PROMPT]

model_type: context_transformer

# Base model from HuggingFace
model_name: "distilroberta-base"

# Random seed for reproducibility
seed: 42

# Context augmentation parameters
include_schema: true
include_description: true
max_schema_length: 200

# Truncation strategy
# -------------------
# "naive"       — concatenate all fields, right-truncate to max_length.
#                  Silently clips the prompt when tool schemas are large.
# "keep_prompt" — reserve prompt_min_tokens for the prompt tail,
#                  truncate context to fit the remainder. Preserves
#                  the attacker-controlled signal under fixed token budgets.
truncate_strategy: "keep_prompt"

# Minimum tokens guaranteed for the prompt (only used with keep_prompt)
prompt_min_tokens: 128

# Which end of the prompt to keep: "tail" (last N tokens, best for
# attacks which often append payloads) or "head" (first N tokens)
prompt_side: "tail"

# Tokenization parameters
max_length: 256

# Training parameters
batch_size: 8
learning_rate: 2.0e-5
num_epochs: 3
warmup_steps: 20
weight_decay: 0.01

# Evaluation strategy
eval_strategy: "epoch"
save_strategy: "epoch"
load_best_model_at_end: true
metric_for_best_model: "roc_auc"

# early_stopping_patience: 1   # uncomment only if your trainer supports it
