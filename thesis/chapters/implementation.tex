\chapter{Implementation}
\label{sec:implementation}

This chapter describes the technical implementation of ToolShield, including the command-line interface, dataset pipeline, training and evaluation infrastructure, experiment organization, and reproducibility measures. The implementation is designed to support controlled, repeatable experiments and to produce thesis-ready artifacts.

\section{CLI Architecture}
\label{subsec:cli_architecture}

ToolShield is implemented as a Python package (\texttt{toolshield}) with a unified command-line interface. The CLI provides subcommands that map directly to pipeline stages:

\begin{itemize}
    \item \texttt{toolshield generate}: produces a labeled JSONL dataset from configured templates and attack families.
    \item \texttt{toolshield split}: generates train/val/test splits under a specified protocol (\texttt{S\_random}, \texttt{S\_attack\_holdout}, or \texttt{S\_tool\_holdout}).
    \item \texttt{toolshield train}: trains a specified model on a given split directory.
    \item \texttt{toolshield eval}: evaluates a trained model on a test split and writes a structured \texttt{metrics.json} file.
    \item \texttt{toolshield report}: generates combined result tables from evaluation outputs.
    \item \texttt{toolshield info}: displays package version and configuration metadata.
\end{itemize}

Each subcommand accepts configuration via command-line arguments and, where applicable, YAML configuration files stored in the \texttt{configs/} directory. This design ensures that every pipeline stage is individually invocable and parameterized, supporting both interactive use and automated orchestration through Makefile targets.

\section{Dataset Pipeline}
\label{subsec:dataset_pipeline}

\subsection{Dataset generation}

The dataset generation stage produces a JSONL file (\texttt{data/dataset.jsonl}) containing labeled records. Each record includes a user prompt, tool metadata (name, schema, description), role framing, label, and attack family annotation. Generation is controlled by a YAML configuration file (\texttt{configs/dataset.yaml}) specifying the number of samples, attack family distribution, tool set, and random seed.

A companion manifest file (\texttt{data/manifest.json}) is written alongside the dataset, recording sample counts, label distribution, seed, and generation timestamp.

\subsection{Split generation}

The split stage partitions the dataset into \texttt{train.jsonl}, \texttt{val.jsonl}, and \texttt{test.jsonl} under a specified protocol. Split constraints are enforced programmatically:

\begin{itemize}
    \item \texttt{S\_random}: stratified random split with template leakage prevention (no \texttt{template\_id} appears in more than one split).
    \item \texttt{S\_attack\_holdout}: AF4 samples excluded from train/val and placed only in test.
    \item \texttt{S\_tool\_holdout}: all samples of a specified tool excluded from train/val.
\end{itemize}

Each protocol writes its splits to a separate directory (e.g., \texttt{data/splits/S\_random/}).

\subsection{Long-schema inflation}

For the enterprise truncation stress test, the script \texttt{scripts/inflate\_schemas.py} transforms the base dataset by inflating tool schemas to a target character length (approximately 4\,000 characters). The inflated dataset is written to \texttt{data/dataset\_longschema.jsonl}, and subsequent splits are generated from it into \texttt{data/splits\_longschema/}.

The inflation process appends realistic enterprise-style properties (nested objects, enumerated constraints, long descriptions) to each schema, controlled by a fixed random seed for reproducibility.

\section{Training Pipeline}
\label{subsec:training_pipeline}

Training is invoked via \texttt{toolshield train} and configured through per-model YAML files in \texttt{configs/training/}. Each configuration specifies:

\begin{itemize}
    \item model type (e.g., \texttt{heuristic}, \texttt{tfidf\_lr}, \texttt{context\_transformer}),
    \item hyperparameters (learning rate, batch size, epochs, warmup steps),
    \item truncation strategy (\texttt{naive} or \texttt{keep\_prompt}),
    \item maximum sequence length and schema length constraints.
\end{itemize}

For the truncation ablation, separate configuration files exist for each strategy variant: \texttt{context\_transformer\_naive.yaml} and \texttt{context\_transformer.yaml} (keep\_prompt). Long-schema variants use dedicated configurations with adjusted \texttt{max\_schema\_length} settings (e.g., \texttt{context\_transformer\_naive\_longschema.yaml}).

Trained model artifacts (weights, tokenizer, and configuration) are saved to the experiment output directory alongside evaluation results.

\section{Evaluation and Reporting Pipeline}
\label{subsec:evaluation_pipeline_impl}

\subsection{Metrics computation}

The evaluation stage (\texttt{toolshield eval}) loads a trained model and test split, runs inference, and computes the following metrics:

\begin{itemize}
    \item ROC-AUC and PR-AUC (threshold-independent ranking metrics),
    \item FPR@TPR90 and FPR@TPR95 (operating-point metrics),
    \item ASR reduction at TPR90 and TPR95,
    \item budget-based threshold evaluation at 1\%, 3\%, and 5\% false-positive budgets,
    \item inference latency at P50 and P95 percentiles (warm-start).
\end{itemize}

Results are written to a structured \texttt{metrics.json} file in the model output directory. A degenerate-score warning is included if the model produces fewer than three unique score values, which can indicate training failure or trivial predictions.

\subsection{Aggregation}

The script \texttt{scripts/aggregate\_experiments.py} discovers all \texttt{metrics.json} files under an experiment root directory, parses them, deduplicates by (protocol, model, seed) key, and produces:

\begin{itemize}
    \item \texttt{raw\_results.csv}: per-seed results for all runs,
    \item \texttt{summary.csv}: mean and standard deviation across seeds,
    \item \texttt{results.json}: combined machine-readable output.
\end{itemize}

Aggregation includes validation checks for missing seeds, missing protocols, and absent transformer models, reporting warnings where applicable.

\subsection{LaTeX table generation}

The script \texttt{scripts/generate\_latex\_from\_summary.py} reads \texttt{summary.csv} (and optionally \texttt{truncation\_stats.csv}) and generates LaTeX table source code suitable for direct inclusion in the thesis. Tables are formatted with consistent metric naming and numerical precision.

\subsection{Truncation statistics}

The script \texttt{scripts/report\_truncation\_stats.py} computes per-strategy truncation statistics from the split data, including prompt retention ratios, schema token distributions, and per-sample truncation counts. It produces \texttt{truncation\_stats.csv}, \texttt{truncation\_stats.json}, and retention visualization figures.

\section{Experiment Automation}
\label{subsec:experiment_automation}

\subsection{Ablation runner}

The script \texttt{scripts/run\_truncation\_ablation.py} orchestrates the full truncation ablation study. It iterates over specified seeds, protocols, and model configurations, invoking training and evaluation for each combination. The script accepts parameters for experiment tag, splits directory, and model set, enabling reuse across standard and long-schema experiments.

\subsection{Makefile targets}

The project \texttt{Makefile} provides targets that compose the full pipeline. Key targets include:

\begin{itemize}
    \item \texttt{make compare\_truncation\_longschema}: executes the complete long-schema stress test (data generation, splitting, training, evaluation, aggregation, truncation statistics, split verification, appendix example export, LaTeX table generation, and run manifest).
    \item \texttt{make verify\_longschema\_results}: runs the automated verification script against the generated artifacts.
    \item \texttt{make test}: executes the full test suite.
\end{itemize}

\section{Experiment Folder Structure}
\label{subsec:folder_structure}

Experiment outputs follow a hierarchical directory structure organized by seed, protocol, and model:

\begin{verbatim}
data/reports/experiments_longschema/
+-- seed_0/
|   +-- S_random/
|   |   +-- context_transformer_naive_longschema/
|   |   |   +-- model/
|   |   |   +-- tokenizer/
|   |   |   +-- metrics.json
|   |   +-- context_transformer_keep_prompt_longschema/
|   |       +-- model/
|   |       +-- tokenizer/
|   |       +-- metrics.json
|   +-- S_attack_holdout/
|       +-- ...
+-- seed_1/
|   +-- ...
+-- seed_2/
|   +-- ...
+-- summary.csv
+-- raw_results.csv
+-- results.json
+-- truncation_stats.csv
+-- truncation_stats.json
+-- split_hygiene.md
+-- guards.json
+-- appendix_truncation_example.md
+-- verification_output.txt
+-- run_manifest.md
+-- latex_tables.txt
\end{verbatim}

This structure enables the aggregation script to discover all per-seed results automatically by traversing the directory tree. Aggregated outputs and verification artifacts are placed at the experiment root level.

\section{Reproducibility Measures}
\label{subsec:reproducibility_impl}

\subsection{Seed control}

All stochastic operations use explicit seeds. Dataset generation uses a configurable seed (default: 1337). Split generation uses a separate seed (default: 2026). Model training seeds are passed as command-line arguments (typically 0, 1, 2). Seeds propagate to Python's \texttt{random} module, NumPy, and PyTorch to ensure deterministic behavior.

\subsection{Deterministic splits}

Split generation enforces template-level disjointness: no \texttt{template\_id} appears in more than one split. For \texttt{S\_attack\_holdout}, AF4 samples are deterministically assigned to test. The split verification script \texttt{scripts/verify\_longschema\_splits.py} checks these constraints and produces \texttt{split\_hygiene.md}, \texttt{split\_hygiene.csv}, and \texttt{guards.json}.

\subsection{Run manifests and environment freeze}

The script \texttt{scripts/write\_run\_manifest.py} captures the exact git commit hash, executed commands, dataset configuration, and model settings at experiment time. The resulting \texttt{run\_manifest.md} provides a complete provenance record.

The Python environment is captured via \texttt{pip freeze} and saved to \texttt{data/reports/environment\_freeze.txt}, recording exact package versions (torch, transformers, scikit-learn, etc.).

\subsection{Verification scripts}

Two verification scripts ensure experimental integrity:

\begin{itemize}
    \item \texttt{scripts/verify\_longschema\_splits.py}: checks template leakage, AF4 holdout correctness, and class balance across all splits. Produces human-readable reports and machine-readable guard files.
    \item \texttt{scripts/verify\_experiments\_longschema\_bundle.py}: validates the complete experiment bundle by checking that all required artifact files exist, that \texttt{summary.csv} contains expected models with correct seed counts, that ROC-AUC values satisfy regression bounds (keep\_prompt $\geq 0.95$, naive $\leq 0.60$), that truncation statistics match expected patterns (naive: 100\% truncation; keep\_prompt: 0\% truncation), that \texttt{split\_hygiene.md} contains no failures and \texttt{guards.json} is empty, and that raw \texttt{metrics.json} files exist for the reference seed. The script exits with code~1 on any failure and prints diagnostic messages.
\end{itemize}

\section{Artifacts and Experiment Bundle}
\label{subsec:artifacts_bundle}

The file \texttt{THESIS\_ARTIFACTS.md} in the repository root serves as the authoritative artifact index for the thesis. It lists the exact file paths of all artifacts required to reproduce and verify the reported results, organized by purpose:

\begin{enumerate}
    \item \textbf{Aggregated results:} \texttt{summary.csv} containing per-protocol, per-model aggregated metrics.
    \item \textbf{Truncation mechanism proof:} \texttt{truncation\_stats.csv} with per-strategy prompt retention ratios and truncation counts.
    \item \textbf{Split hygiene:} \texttt{split\_hygiene.md} (human-readable check results) and \texttt{guards.json} (machine-readable warnings).
    \item \textbf{Concrete appendix example:} \texttt{appendix\_truncation\_example.md} providing a token-level walkthrough of truncation for a single record under both strategies.
    \item \textbf{Verification output:} \texttt{verification\_output.txt} documenting the terminal output of the automated verification run.
    \item \textbf{Reproducibility:} \texttt{run\_manifest.md} (git hash, commands, seeds, configuration) and \texttt{environment\_freeze.txt} (pip freeze snapshot).
\end{enumerate}

This artifact index establishes a contract between the thesis text and the repository: every numerical claim in the Results and Discussion chapters can be traced to a specific file listed in \texttt{THESIS\_ARTIFACTS.md}. The complete experiment bundle can be regenerated from a clean state using:

\begin{verbatim}
make clean
make compare_truncation_longschema
make verify_longschema_results
\end{verbatim}

Successful execution of \texttt{make verify\_longschema\_results} confirms that all artifacts are present and internally consistent.
